{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anomaly_4_classes_using imageclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogaveera/atmanomalydetection/blob/master/anomaly_4_classes_using_imageclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6jMMEcdS5B3",
        "colab_type": "code",
        "outputId": "1b2c9449-e856-4b25-ce35-672bd3bfe20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTt3IU9RC8nB",
        "colab_type": "code",
        "outputId": "b38ed565-d230-4922-de71-68fcfd2b5729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "train_video_index = []\n",
        "test_video_index = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMjx_CNDGIt",
        "colab_type": "code",
        "outputId": "acec1f30-4581-4dbc-eb89-a18da4970249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/Mogaveera/anomalydetection.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'anomalydetection'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 250 (delta 0), reused 5 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (250/250), 733.86 MiB | 38.45 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "Checking out files: 100% (233/233), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1jogWNDKUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Get model with pretrained weights.\n",
        "    # base_model = InceptionV3(\n",
        "    # weights='imagenet',\n",
        "    # include_top=True)\n",
        "    \n",
        "    \n",
        "    # We'll extract features at the final pool layer.\n",
        "    # model = Model(\n",
        "    #     inputs=base_model.input,\n",
        "    #     outputs=base_model.get_layer('avg_pool').output)\n",
        "    # # model.save('inception.h5')\n",
        "\n",
        "    base_model = load_model('./drive/My Drive/imageclassifier3_1.h5')\n",
        "\n",
        "    layer_name = 'flatten_1'\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.get_layer(layer_name).output)\n",
        "\n",
        "    # Getting the data\n",
        "    df = get_data('anomalydetection/Data/data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    # df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    train_video_index = make_dataset(train, model,label_index, \"train\")\n",
        "    test_video_index = make_dataset(test, model,label_index,\"test\")\n",
        "    return (train_video_index, test_video_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLx9uiODN6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "#def clean_data(df):\n",
        "#    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "#    df = df[mask]`\n",
        "#    return df\n",
        "\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (299,299))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def video_to_frame(row, model,label_index, phase, not_created):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    index = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(\"anomalydetection/Data\",\"anomaly_dataset\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".mp4\")) \n",
        "    #print(str(row[\"class\"].iloc[0]))\n",
        "    #print(str(row[\"video_name\"].iloc[0]))\n",
        "    frameno = 1\n",
        "    imgs = []\n",
        "    length = 0\n",
        "    seq = 12\n",
        "    while (cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if length < seq:\n",
        "          if frameno % 10 == 0:\n",
        "            frameno = frameno + 1\n",
        "            frame = preprocess_image(frame)\n",
        "            frame = image.img_to_array(frame)\n",
        "            frame = frame / 255\n",
        "            # features = model.predict(frame)\n",
        "            imgs.append(frame)\n",
        "            length = length + 1\n",
        "          else:\n",
        "            frameno = frameno + 1\n",
        "        else:\n",
        "          seq = seq + 12\n",
        "          imgs1 = np.array(imgs)\n",
        "          features = model.predict(imgs1)\n",
        "          input_f.append(features)\n",
        "          output_y.append(label_index)\n",
        "          del imgs[:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if not_created:\n",
        "      f = h5py.File(phase+'_4'+'.h5', 'w')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        f.create_dataset(phase, data=input_f1, maxshape=(None, 12, 2048))\n",
        "        f.create_dataset(phase+\"_labels\", data=output_y1, maxshape=(None,))\n",
        "        f.close()\n",
        "    else:\n",
        "      hf = h5py.File(phase+'_4'+'.h5', 'a')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        hf[phase].resize((hf[phase].shape[0] + input_f1.shape[0]), axis = 0)\n",
        "        hf[phase][-input_f1.shape[0]:] = input_f1\n",
        "\n",
        "        hf[phase+\"_labels\"].resize((hf[phase+\"_labels\"].shape[0] + output_y1.shape[0]), axis = 0)\n",
        "        hf[phase+\"_labels\"][-output_y1.shape[0]:] = output_y1\n",
        "        hf.close()\n",
        "\n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "    del imgs[:]\n",
        "    cap.release()\n",
        "    return index\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def make_dataset(data, model,label_index, phase):\n",
        "    video_index = [0]\n",
        "    required_classes = [\"Arson\",\"Burglary\", \"Fighting\", \"normal\"]\n",
        "   \n",
        "    not_created = True\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        "            index = required_classes.index(str(data.iloc[[i]][\"class\"].iloc[0]))\n",
        "            # label_index = np.zeros((4))\n",
        "            # label_index[index] = 1\n",
        "            label_index = index\n",
        "            index = video_to_frame(data.iloc[[i]], model,label_index, phase, not_created)\n",
        "            real_index = video_index[-1] + index\n",
        "            video_index.append(real_index)\n",
        "            if real_index > 0:\n",
        "              not_created = False\n",
        "\n",
        "    return video_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa9JuDGnFJbl",
        "colab_type": "code",
        "outputId": "96baea46-48b1-4eb9-ceba-60d92ddcd304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "train_video_index, test_video_index = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 192/192 [01:10<00:00,  1.61it/s]\n",
            "100%|██████████| 39/39 [00:18<00:00,  1.15s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZtdRrCFKRp",
        "colab_type": "code",
        "outputId": "aef26f39-67b0-43ab-d19d-c1d543855489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "x_train = HDF5Matrix('train_4.h5', 'train')\n",
        "y_train = HDF5Matrix('train_4.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_4.h5', 'test')\n",
        "y_test = HDF5Matrix('test_4.h5', 'test_labels')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[783])\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test[93])\n",
        "print(train_video_index)\n",
        "print(test_video_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(786, 12, 2048)\n",
            "(786,)\n",
            "3\n",
            "(225, 12, 2048)\n",
            "(225,)\n",
            "3\n",
            "[0, 0, 3, 6, 10, 13, 16, 20, 21, 23, 26, 29, 32, 37, 42, 46, 49, 57, 61, 64, 68, 68, 73, 78, 83, 86, 90, 94, 96, 99, 101, 104, 105, 106, 107, 112, 114, 118, 123, 124, 128, 131, 132, 134, 136, 145, 150, 156, 160, 164, 167, 171, 173, 176, 177, 180, 197, 198, 208, 211, 222, 226, 241, 247, 248, 259, 277, 283, 292, 295, 330, 348, 374, 390, 423, 428, 435, 436, 453, 457, 477, 482, 485, 504, 513, 519, 522, 530, 542, 545, 554, 577, 585, 589, 598, 611, 621, 631, 637, 650, 658, 677, 689, 704, 706, 714, 722, 727, 730, 737, 751, 753, 759, 775, 780, 786]\n",
            "[0, 1, 3, 4, 5, 9, 12, 17, 20, 26, 33, 38, 45, 52, 64, 69, 87, 93, 108, 165, 206, 215, 218, 225]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzPj0HpcTVQ6",
        "colab_type": "code",
        "outputId": "18a57d72-5a1d-4eb3-adaf-3b485567b5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_test1 = np.array(y_test)\n",
        "print(y_test1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdbPFBLhFP2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predominantly.\"\"\"\n",
        "    input_shape = (12, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(2048), input_shape=input_shape))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    #model.add(Dense(10, activation='softmax'))\"\"\"\n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "    \n",
        "    callback_list = [checkpoint]\n",
        "    \n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "    #return model, callback_list\n",
        "    #model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "    #          loss = 'categorical_crossentropy',\n",
        "    #        metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdZFdLaqWk5T",
        "colab_type": "code",
        "outputId": "007f8e36-df91-4f78-fa89-3f716506f198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "lstm = lstm()\n",
        "lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 4096)              67125248  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 69,224,964\n",
            "Trainable params: 69,224,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZXBVoGWqz5",
        "colab_type": "code",
        "outputId": "2e460f60-a83a-421e-b033-1d0d2038fe61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "lstm.fit(x_train, y_train, batch_size = 16, epochs = 25,verbose = 2, validation_data = (x_test, y_test), shuffle = 'batch')\n",
        "lstm.save(\"Anomaly_Recognition_4classes.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 786 samples, validate on 225 samples\n",
            "Epoch 1/25\n",
            " - 10s - loss: 2.4261 - acc: 0.3104 - val_loss: 1.2134 - val_acc: 0.3333\n",
            "Epoch 2/25\n",
            " - 5s - loss: 1.9737 - acc: 0.3702 - val_loss: 1.2494 - val_acc: 0.2667\n",
            "Epoch 3/25\n",
            " - 5s - loss: 1.6700 - acc: 0.4338 - val_loss: 1.0971 - val_acc: 0.4756\n",
            "Epoch 4/25\n",
            " - 5s - loss: 1.5592 - acc: 0.4580 - val_loss: 1.1776 - val_acc: 0.2756\n",
            "Epoch 5/25\n",
            " - 5s - loss: 1.4597 - acc: 0.4389 - val_loss: 1.1485 - val_acc: 0.2978\n",
            "Epoch 6/25\n",
            " - 5s - loss: 1.3596 - acc: 0.4936 - val_loss: 0.9880 - val_acc: 0.5333\n",
            "Epoch 7/25\n",
            " - 5s - loss: 1.2557 - acc: 0.5153 - val_loss: 1.0302 - val_acc: 0.5244\n",
            "Epoch 8/25\n",
            " - 5s - loss: 1.2088 - acc: 0.5394 - val_loss: 1.0782 - val_acc: 0.4044\n",
            "Epoch 9/25\n",
            " - 5s - loss: 1.2596 - acc: 0.5293 - val_loss: 0.9629 - val_acc: 0.6178\n",
            "Epoch 10/25\n",
            " - 5s - loss: 1.1599 - acc: 0.5420 - val_loss: 1.0526 - val_acc: 0.3956\n",
            "Epoch 11/25\n",
            " - 5s - loss: 1.0814 - acc: 0.5865 - val_loss: 1.0111 - val_acc: 0.4978\n",
            "Epoch 12/25\n",
            " - 5s - loss: 1.1168 - acc: 0.5827 - val_loss: 0.9611 - val_acc: 0.7200\n",
            "Epoch 13/25\n",
            " - 5s - loss: 1.0699 - acc: 0.5700 - val_loss: 1.0593 - val_acc: 0.4311\n",
            "Epoch 14/25\n",
            " - 5s - loss: 1.0149 - acc: 0.6234 - val_loss: 0.9989 - val_acc: 0.5600\n",
            "Epoch 15/25\n",
            " - 5s - loss: 1.0109 - acc: 0.5967 - val_loss: 0.9679 - val_acc: 0.7333\n",
            "Epoch 16/25\n",
            " - 5s - loss: 0.9240 - acc: 0.6501 - val_loss: 0.9404 - val_acc: 0.6444\n",
            "Epoch 17/25\n",
            " - 5s - loss: 0.9994 - acc: 0.6349 - val_loss: 0.9081 - val_acc: 0.7467\n",
            "Epoch 18/25\n",
            " - 5s - loss: 0.8708 - acc: 0.6845 - val_loss: 0.9064 - val_acc: 0.7156\n",
            "Epoch 19/25\n",
            " - 5s - loss: 0.8182 - acc: 0.6934 - val_loss: 1.0061 - val_acc: 0.4889\n",
            "Epoch 20/25\n",
            " - 5s - loss: 0.8597 - acc: 0.6730 - val_loss: 0.9283 - val_acc: 0.7244\n",
            "Epoch 21/25\n",
            " - 5s - loss: 0.8098 - acc: 0.6908 - val_loss: 0.8906 - val_acc: 0.7244\n",
            "Epoch 22/25\n",
            " - 5s - loss: 0.8077 - acc: 0.7010 - val_loss: 0.9845 - val_acc: 0.5911\n",
            "Epoch 23/25\n",
            " - 5s - loss: 0.7632 - acc: 0.7316 - val_loss: 0.9056 - val_acc: 0.7200\n",
            "Epoch 24/25\n",
            " - 5s - loss: 0.6981 - acc: 0.7557 - val_loss: 0.9127 - val_acc: 0.7200\n",
            "Epoch 25/25\n",
            " - 5s - loss: 0.7188 - acc: 0.7303 - val_loss: 0.9041 - val_acc: 0.7156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdRDgRq0Wxcp",
        "colab_type": "code",
        "outputId": "fde6a06c-adbc-4839-8f05-cde99a181cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = lstm.evaluate(x_test, y_test, batch_size=16)\n",
        "print(\"accuracy: \", test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 0s 2ms/step\n",
            "accuracy:  0.7155555555555555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCWp-Fv5lgdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}