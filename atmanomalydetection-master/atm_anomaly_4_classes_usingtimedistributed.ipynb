{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atm_anomaly_4_classes_usingtimedistributed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogaveera/atmanomalydetection/blob/master/atm_anomaly_4_classes_usingtimedistributed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFDa3zVm9-xc",
        "colab_type": "code",
        "outputId": "4e28eea7-59b9-4654-e7e5-ac820301e721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout, Flatten, ZeroPadding2D,Reshape\n",
        "from keras.layers.convolutional import (Conv2D, MaxPooling2D, AveragePooling2D, Conv3D, MaxPooling3D)\n",
        "from keras.layers import Bidirectional, TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "from keras.utils import np_utils\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "train_video_index = []\n",
        "test_video_index = []"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-aCxukO_Evr",
        "colab_type": "code",
        "outputId": "268d3ee8-2866-448e-af4c-eb8cd6d4aa92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/Mogaveera/atmanomaly.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'atmanomaly'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\n",
            "Receiving objects: 100% (159/159), 203.93 MiB | 37.93 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Checking out files: 100% (151/151), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHpHp2H3_Jna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Getting the data\n",
        "    df = get_data('atmanomaly/Data/data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    #df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    train_video_index = make_dataset(train, label_index, \"train\")\n",
        "    test_video_index = make_dataset(test, label_index,\"test\")\n",
        "    return (train_video_index, test_video_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwMP826Q_P17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "#def clean_data(df):\n",
        "#    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "#    df = df[mask]`\n",
        "#    return df\n",
        "\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (299,299))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def video_to_frame(row, label_index, phase, not_created):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    index = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(\"atmanomaly/Data\",\"anomaly_dataset\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".mp4\")) \n",
        "    #print(str(row[\"class\"].iloc[0]))\n",
        "    #print(str(row[\"video_name\"].iloc[0]))\n",
        "    frameno = 1\n",
        "    imgs = []\n",
        "    length = 0\n",
        "    seq = 12\n",
        "    while (cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if length < seq:\n",
        "          if frameno % 10 == 0:\n",
        "            frameno = frameno + 1\n",
        "            frame = preprocess_image(frame)\n",
        "            frame = image.img_to_array(frame)\n",
        "            frame = frame / 255\n",
        "            imgs.append(frame)\n",
        "            length = length + 1\n",
        "          else:\n",
        "            frameno = frameno + 1\n",
        "        else:\n",
        "          seq = seq + 12\n",
        "          imgs1 = np.array(imgs)\n",
        "          input_f.append(imgs1)\n",
        "          output_y.append(label_index)\n",
        "          del imgs[:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "\n",
        "    if not_created == True:\n",
        "      f = h5py.File(phase+'_4'+'.h5', 'w')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        f.create_dataset(phase, data=input_f1, maxshape=(None, 12, 299, 299, 3))\n",
        "        f.create_dataset(phase+\"_labels\", data=output_y1, maxshape=(None, 4))\n",
        "        f.close()\n",
        "    else:\n",
        "      hf = h5py.File(phase+'_4'+'.h5', 'a')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        hf[phase].resize((hf[phase].shape[0] + input_f1.shape[0]), axis = 0)\n",
        "        hf[phase][-input_f1.shape[0]:] = input_f1\n",
        "\n",
        "        hf[phase+\"_labels\"].resize((hf[phase+\"_labels\"].shape[0] + output_y1.shape[0]), axis = 0)\n",
        "        hf[phase+\"_labels\"][-output_y1.shape[0]:] = output_y1\n",
        "        hf.close()\n",
        "\n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "    del imgs[:]\n",
        "    cap.release()\n",
        "    return index\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def make_dataset(data, label_index, phase):\n",
        "    video_index = [0]\n",
        "    required_classes = [\"Arson\", \"Burglary\", \"Fighting\", \"normal\"]\n",
        "   \n",
        "    not_created = True\n",
        "\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        "            index = required_classes.index(str(data.iloc[[i]][\"class\"].iloc[0]))\n",
        "            label_index = np.zeros((4))\n",
        "            label_index[index] = 1\n",
        "            index = video_to_frame(data.iloc[[i]], label_index, phase, not_created)\n",
        "            real_index = video_index[-1] + index\n",
        "            video_index.append(real_index)\n",
        "            if real_index > 0:\n",
        "              not_created = False\n",
        "\n",
        "    return video_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgI3RrTU_a0b",
        "colab_type": "code",
        "outputId": "de22b9f5-8838-4791-db69-a8bbeb50066e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_video_index, test_video_index = main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [03:23<00:00,  1.71s/it]\n",
            "100%|██████████| 24/24 [00:57<00:00,  3.18s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpTiFGRa_em3",
        "colab_type": "code",
        "outputId": "92f0fea5-24a4-426f-bdb7-605d4717b6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "x_train = HDF5Matrix('train_4.h5', 'train')\n",
        "y_train = HDF5Matrix('train_4.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_4.h5', 'test')\n",
        "y_test = HDF5Matrix('test_4.h5', 'test_labels')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[210])\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test[3])\n",
        "print(train_video_index)\n",
        "print(test_video_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(702, 12, 299, 299, 3)\n",
            "(702, 4)\n",
            "[0. 1. 0. 0.]\n",
            "(176, 12, 299, 299, 3)\n",
            "(176, 4)\n",
            "[1. 0. 0. 0.]\n",
            "[0, 3, 6, 10, 13, 16, 20, 21, 23, 26, 29, 32, 37, 42, 46, 49, 57, 61, 64, 68, 68, 73, 78, 83, 86, 90, 94, 96, 99, 101, 104, 105, 106, 107, 112, 114, 118, 123, 124, 128, 131, 132, 134, 136, 145, 150, 156, 160, 164, 167, 171, 174, 180, 187, 192, 198, 217, 221, 244, 266, 269, 271, 278, 280, 282, 285, 286, 289, 295, 296, 298, 301, 312, 316, 327, 333, 334, 345, 363, 369, 378, 381, 387, 390, 398, 412, 417, 424, 425, 429, 434, 440, 445, 448, 457, 461, 470, 477, 485, 492, 499, 504, 514, 521, 529, 545, 550, 558, 565, 569, 578, 585, 592, 600, 610, 628, 641, 648, 656, 666, 672, 681, 685, 693, 696, 702]\n",
            "[0, 1, 3, 4, 5, 9, 12, 17, 21, 42, 48, 60, 61, 68, 75, 87, 92, 110, 116, 134, 140, 150, 157, 163, 176]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5lWvSLa_hhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model():\n",
        "    input_shape = (12 ,299, 299, 3)\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(64, 3, 3, activation='relu', border_mode='valid', name='conv1', subsample=(2, 2)), input_shape=input_shape))\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='valid', name='pool1')))\n",
        "      \n",
        "    model.add(TimeDistributed(Conv2D(128, 3, 3, activation='relu', border_mode='valid', name='conv2', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='valid', name='pool2')))    \n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(256, 3, 3, activation='relu', border_mode='valid', name='conv3a', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(Conv2D(256, 3, 3, activation='relu', border_mode='valid', name='conv3b', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='valid', name='pool3')))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(512, 3, 3, activation='relu', border_mode='valid', name='conv4a', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(Conv2D(512, 3, 3, activation='relu', border_mode='valid', name='conv4b', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='valid', name='pool4')))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(512, 3, 3, activation='relu', border_mode='valid', name='conv5a', subsample=(1, 1))))\n",
        "    model.add(TimeDistributed(Conv2D(512, 3, 3, activation='relu', border_mode='valid', name='conv5a', subsample=(1, 1))))\n",
        "    #model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='valid', name='pool5')))\n",
        "    \n",
        "    # model.add(Dense(2048, activation='relu', name='fc7'))\n",
        "    \n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(512)))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', name='fc9'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "\n",
        "    callback_list = [checkpoint]\n",
        "    \n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    #return model, callback_list\n",
        "    #model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "    #          loss = 'categorical_crossentropy',\n",
        "    #        metrics=['accuracy'])\n",
        "    return model, callback_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qSx65VH_rvI",
        "colab_type": "code",
        "outputId": "5bdb7ae7-8f24-4dd3-997d-8a2c45e8ef44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, callback_list = model()\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1\", strides=(2, 2), padding=\"valid\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"pool1\", padding=\"valid\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2\", strides=(1, 1), padding=\"valid\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"pool2\", padding=\"valid\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3a\", strides=(1, 1), padding=\"valid\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3b\", strides=(1, 1), padding=\"valid\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"pool3\", padding=\"valid\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4a\", strides=(1, 1), padding=\"valid\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4b\", strides=(1, 1), padding=\"valid\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"pool4\", padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5a\", strides=(1, 1), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5a\", strides=(1, 1), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"pool5\", padding=\"valid\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 12, 149, 149, 64)  1792      \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 12, 74, 74, 64)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 12, 72, 72, 128)   73856     \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 12, 36, 36, 128)   0         \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 12, 34, 34, 256)   295168    \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 12, 32, 32, 256)   590080    \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 12, 16, 16, 256)   0         \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 12, 14, 14, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 12, 12, 12, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 12, 6, 6, 512)     0         \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 12, 4, 4, 512)     2359808   \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, 12, 2, 2, 512)     2359808   \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, 12, 1, 1, 512)     0         \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, 12, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1024)              4198400   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc9 (Dense)                  (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 13,945,732\n",
            "Trainable params: 13,945,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1cwKUR_u97",
        "colab_type": "code",
        "outputId": "1f9e6451-9583-4df2-cbf3-8befa314808b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 6, epochs = 20,verbose = 2, validation_data = (x_test, y_test), shuffle = 'batch', callbacks=callback_list)\n",
        "model.save(\"atm_anomaly_4_bytimedistibuted.h5\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 702 samples, validate on 176 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 333s - loss: 1.3829 - acc: 0.3034 - val_loss: 1.3675 - val_acc: 0.3409\n",
            "Epoch 2/20\n",
            " - 302s - loss: 1.3673 - acc: 0.3504 - val_loss: 1.3078 - val_acc: 0.3409\n",
            "Epoch 3/20\n",
            " - 290s - loss: 1.3485 - acc: 0.3490 - val_loss: 1.3029 - val_acc: 0.3409\n",
            "Epoch 4/20\n",
            " - 275s - loss: 1.3367 - acc: 0.3447 - val_loss: 1.2996 - val_acc: 0.3409\n",
            "Epoch 5/20\n",
            " - 274s - loss: 1.2985 - acc: 0.3575 - val_loss: 1.2921 - val_acc: 0.3580\n",
            "Epoch 6/20\n",
            " - 266s - loss: 1.2493 - acc: 0.3960 - val_loss: 1.2063 - val_acc: 0.3466\n",
            "Epoch 7/20\n",
            " - 265s - loss: 1.1547 - acc: 0.4915 - val_loss: 1.1632 - val_acc: 0.5625\n",
            "Epoch 8/20\n",
            " - 278s - loss: 1.0346 - acc: 0.5413 - val_loss: 0.9416 - val_acc: 0.7216\n",
            "Epoch 9/20\n",
            " - 274s - loss: 0.9270 - acc: 0.6197 - val_loss: 0.8895 - val_acc: 0.7273\n",
            "Epoch 10/20\n",
            " - 274s - loss: 0.8394 - acc: 0.6610 - val_loss: 0.7919 - val_acc: 0.7955\n",
            "Epoch 11/20\n",
            " - 276s - loss: 0.7929 - acc: 0.7051 - val_loss: 0.8223 - val_acc: 0.7841\n",
            "Epoch 12/20\n",
            " - 272s - loss: 0.7286 - acc: 0.7123 - val_loss: 0.7401 - val_acc: 0.8011\n",
            "Epoch 13/20\n",
            " - 279s - loss: 0.6957 - acc: 0.7450 - val_loss: 0.6637 - val_acc: 0.7386\n",
            "Epoch 14/20\n",
            " - 283s - loss: 0.6487 - acc: 0.7678 - val_loss: 0.7550 - val_acc: 0.6875\n",
            "Epoch 15/20\n",
            " - 278s - loss: 0.6246 - acc: 0.7678 - val_loss: 0.7820 - val_acc: 0.6932\n",
            "Epoch 16/20\n",
            " - 276s - loss: 0.5713 - acc: 0.7863 - val_loss: 0.6140 - val_acc: 0.7955\n",
            "Epoch 17/20\n",
            " - 277s - loss: 0.5243 - acc: 0.8034 - val_loss: 0.8245 - val_acc: 0.7670\n",
            "Epoch 18/20\n",
            " - 275s - loss: 0.4725 - acc: 0.8262 - val_loss: 0.6517 - val_acc: 0.7557\n",
            "Epoch 19/20\n",
            " - 279s - loss: 0.4374 - acc: 0.8348 - val_loss: 0.6618 - val_acc: 0.7955\n",
            "Epoch 20/20\n",
            " - 281s - loss: 0.3834 - acc: 0.8661 - val_loss: 0.7150 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndlny9YB_66Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model, callback_list = model()\n",
        "# model.load_weights('atm_anomaly_4_bytimedistibuted.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh8CDHTj_9K8",
        "colab_type": "code",
        "outputId": "3dd1b3e5-df03-45f9-8fcb-7b9071276c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, batch_size = 6)\n",
        "print(\"accuracy: \", test_acc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 7s 38ms/step\n",
            "accuracy:  0.7500000032172962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpYTzb35WrNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}