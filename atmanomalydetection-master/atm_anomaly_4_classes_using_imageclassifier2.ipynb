{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atm_anomaly_4_classes_using imageclassifier2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogaveera/atmanomalydetection/blob/master/atm_anomaly_4_classes_using_imageclassifier2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVfedpOwSskD",
        "colab_type": "code",
        "outputId": "ad31f9f0-ab7e-4664-ffc6-2cf5fe04f923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTt3IU9RC8nB",
        "colab_type": "code",
        "outputId": "4fc74e89-9326-4394-c14a-7bfed84833aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "from keras.models import load_model\n",
        "\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "train_video_index = []\n",
        "test_video_index = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMjx_CNDGIt",
        "colab_type": "code",
        "outputId": "28f11fbd-b410-44f4-d277-ee741fd626bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/Mogaveera/atmanomaly.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'atmanomaly'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (159/159), 203.93 MiB | 48.50 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Checking out files: 100% (151/151), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1jogWNDKUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "\n",
        "    base_model = load_model('./drive/My Drive/imageclassifier3.h5')\n",
        "\n",
        "    layer_name = 'flatten_1'\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.get_layer(layer_name).output)\n",
        "\n",
        "\n",
        "    # Getting the data\n",
        "    df = get_data('atmanomaly/Data/data_file.csv')\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    train_video_index = make_dataset(train, model, \"train\")\n",
        "    test_video_index = make_dataset(test, model,\"test\")\n",
        "    return (train_video_index, test_video_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLx9uiODN6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (299,299))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def video_to_frame(row, model,label_index, phase, not_created):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    index = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(\"atmanomaly/Data\",\"anomaly_dataset\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".mp4\")) \n",
        "    #print(str(row[\"class\"].iloc[0]))\n",
        "    #print(str(row[\"video_name\"].iloc[0]))\n",
        "    frameno = 1\n",
        "    imgs = []\n",
        "    length = 0\n",
        "    seq = 12\n",
        "    while (cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if length < seq:\n",
        "          if frameno % 10 == 0:\n",
        "            frameno = frameno + 1\n",
        "            frame = preprocess_image(frame)\n",
        "            frame = image.img_to_array(frame)\n",
        "            frame = frame / 255.0\n",
        "            # features = model.predict(frame)\n",
        "            imgs.append(frame)\n",
        "            length = length + 1\n",
        "          else:\n",
        "            frameno = frameno + 1\n",
        "        else:\n",
        "          seq = seq + 12\n",
        "          imgs1 = np.array(imgs)\n",
        "          features = model.predict(imgs1)\n",
        "          input_f.append(features)\n",
        "          output_y.append(label_index)\n",
        "          del imgs[:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if not_created:\n",
        "      f = h5py.File(phase+'_4'+'.h5', 'w')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        f.create_dataset(phase, data=input_f1, maxshape=(None, 12, 2048))\n",
        "        f.create_dataset(phase+\"_labels\", data=output_y1, maxshape=(None, ))\n",
        "        f.close()\n",
        "    else:\n",
        "      hf = h5py.File(phase+'_4'+'.h5', 'a')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        hf[phase].resize((hf[phase].shape[0] + input_f1.shape[0]), axis = 0)\n",
        "        hf[phase][-input_f1.shape[0]:] = input_f1\n",
        "\n",
        "        hf[phase+\"_labels\"].resize((hf[phase+\"_labels\"].shape[0] + output_y1.shape[0]), axis = 0)\n",
        "        hf[phase+\"_labels\"][-output_y1.shape[0]:] = output_y1\n",
        "        hf.close()\n",
        "\n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "    del imgs[:]\n",
        "    cap.release()\n",
        "    return index\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def make_dataset(data, model, phase):\n",
        "    video_index = [0]\n",
        "    required_classes = [\"Arson\", \"Burglary\", \"Fighting\", \"normal\"]\n",
        "   \n",
        "    not_created = True\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        "            index = required_classes.index(str(data.iloc[[i]][\"class\"].iloc[0]))\n",
        "            # label_index = np.zeros((4))\n",
        "            # label_index[index] = 1\n",
        "            label_index = index\n",
        "            index = video_to_frame(data.iloc[[i]], model,label_index, phase, not_created)\n",
        "            real_index = video_index[-1] + index\n",
        "            video_index.append(real_index)\n",
        "            if real_index > 0:\n",
        "              not_created = False\n",
        "\n",
        "    return video_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa9JuDGnFJbl",
        "colab_type": "code",
        "outputId": "505295c9-a1a8-44a0-db26-b18acff149fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "train_video_index, test_video_index = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [01:20<00:00,  1.60it/s]\n",
            "100%|██████████| 24/24 [00:18<00:00,  1.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZtdRrCFKRp",
        "colab_type": "code",
        "outputId": "012cb107-1001-4a98-df07-51417785ab07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "x_train = HDF5Matrix('train_4.h5', 'train')\n",
        "y_train = HDF5Matrix('train_4.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_4.h5', 'test')\n",
        "y_test = HDF5Matrix('test_4.h5', 'test_labels')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[:10])\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test[116])\n",
        "print(train_video_index)\n",
        "print(test_video_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(702, 12, 2048)\n",
            "(702,)\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "(176, 12, 2048)\n",
            "(176,)\n",
            "3\n",
            "[0, 3, 6, 10, 13, 16, 20, 21, 23, 26, 29, 32, 37, 42, 46, 49, 57, 61, 64, 68, 68, 73, 78, 83, 86, 90, 94, 96, 99, 101, 104, 105, 106, 107, 112, 114, 118, 123, 124, 128, 131, 132, 134, 136, 145, 150, 156, 160, 164, 167, 171, 174, 180, 187, 192, 198, 217, 221, 244, 266, 269, 271, 278, 280, 282, 285, 286, 289, 295, 296, 298, 301, 312, 316, 327, 333, 334, 345, 363, 369, 378, 381, 387, 390, 398, 412, 417, 424, 425, 429, 434, 440, 445, 448, 457, 461, 470, 477, 485, 492, 499, 504, 514, 521, 529, 545, 550, 558, 565, 569, 578, 585, 592, 600, 610, 628, 641, 648, 656, 666, 672, 681, 685, 693, 696, 702]\n",
            "[0, 1, 3, 4, 5, 9, 12, 17, 21, 42, 48, 60, 61, 68, 75, 87, 92, 110, 116, 134, 140, 150, 157, 163, 176]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q57SEB7nc_19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train1 = np.array(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vio1V6WI_1f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test1 = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W69k_lfAhV-",
        "colab_type": "code",
        "outputId": "0f6663e7-4c76-4c54-f767-e2101d650a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(y_test1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdbPFBLhFP2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predominantly.\"\"\"\n",
        "    input_shape = (12, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(2048), input_shape=input_shape))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    #model.add(Dense(10, activation='softmax'))\"\"\"\n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "    \n",
        "    # callback_list = [checkpoint]\n",
        "    \n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "    #return model, callback_list\n",
        "    #model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "    #          loss = 'categorical_crossentropy',\n",
        "    #        metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdZFdLaqWk5T",
        "colab_type": "code",
        "outputId": "b8a14bfd-c234-41a9-a77d-5e7eac3f7ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "lstm = lstm()\n",
        "lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 4096)              67125248  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 69,224,964\n",
            "Trainable params: 69,224,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZXBVoGWqz5",
        "colab_type": "code",
        "outputId": "c931efdb-2027-40cb-b152-073b49e31aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "lstm.fit(x_train, y_train, batch_size = 16, epochs = 20,verbose = 2, shuffle = 'batch')\n",
        "lstm.save(\"atm_anomaly_4classes_inceptionv3.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " - 9s - loss: 1.3923 - acc: 0.3775\n",
            "Epoch 2/20\n",
            " - 4s - loss: 0.9240 - acc: 0.6239\n",
            "Epoch 3/20\n",
            " - 4s - loss: 0.8213 - acc: 0.6695\n",
            "Epoch 4/20\n",
            " - 4s - loss: 0.6648 - acc: 0.7521\n",
            "Epoch 5/20\n",
            " - 4s - loss: 0.5109 - acc: 0.7963\n",
            "Epoch 6/20\n",
            " - 4s - loss: 0.3935 - acc: 0.8547\n",
            "Epoch 7/20\n",
            " - 4s - loss: 0.3281 - acc: 0.8989\n",
            "Epoch 8/20\n",
            " - 4s - loss: 0.2792 - acc: 0.9017\n",
            "Epoch 9/20\n",
            " - 4s - loss: 0.2829 - acc: 0.8946\n",
            "Epoch 10/20\n",
            " - 4s - loss: 0.2428 - acc: 0.9245\n",
            "Epoch 11/20\n",
            " - 4s - loss: 0.2112 - acc: 0.9302\n",
            "Epoch 12/20\n",
            " - 4s - loss: 0.1705 - acc: 0.9587\n",
            "Epoch 13/20\n",
            " - 4s - loss: 0.1385 - acc: 0.9615\n",
            "Epoch 14/20\n",
            " - 4s - loss: 0.1109 - acc: 0.9658\n",
            "Epoch 15/20\n",
            " - 4s - loss: 0.1369 - acc: 0.9544\n",
            "Epoch 16/20\n",
            " - 4s - loss: 0.0925 - acc: 0.9758\n",
            "Epoch 17/20\n",
            " - 4s - loss: 0.0649 - acc: 0.9886\n",
            "Epoch 18/20\n",
            " - 4s - loss: 0.0561 - acc: 0.9872\n",
            "Epoch 19/20\n",
            " - 4s - loss: 0.0475 - acc: 0.9915\n",
            "Epoch 20/20\n",
            " - 4s - loss: 0.0455 - acc: 0.9929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdRDgRq0Wxcp",
        "colab_type": "code",
        "outputId": "9e5af82c-5cdf-4aeb-9898-bdcd3277547a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = lstm.evaluate(x_test, y_test, batch_size=16)\n",
        "print(\"accuracy: \", test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 0s 2ms/step\n",
            "accuracy:  0.8636363636363636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCWp-Fv5lgdG",
        "colab_type": "code",
        "outputId": "56a053db-34f4-44e9-edc2-41c1e955ec67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "imgs = []\n",
        "imgs.append(x_test[116])\n",
        "imgs1 = np.array(imgs)\n",
        "\n",
        "pred = lstm.predict_classes(imgs1)\n",
        "\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM3VUQhUlH97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}