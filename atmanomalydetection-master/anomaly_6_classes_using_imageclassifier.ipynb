{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anomaly_6_classes_using imageclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogaveera/atmanomalydetection/blob/master/anomaly_6_classes_using_imageclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6jMMEcdS5B3",
        "colab_type": "code",
        "outputId": "c3e856e1-ac23-499e-b1f7-62f75e2f8e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTt3IU9RC8nB",
        "colab_type": "code",
        "outputId": "5fca432f-c055-4f9f-ac29-bfee26d86b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "train_video_index = []\n",
        "test_video_index = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMjx_CNDGIt",
        "colab_type": "code",
        "outputId": "272b686d-4b53-4c23-888f-6ffc92ee6a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/Mogaveera/anomalydetection.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'anomalydetection'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Total 250 (delta 0), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (250/250), 733.86 MiB | 49.05 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "Checking out files: 100% (233/233), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1jogWNDKUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Get model with pretrained weights.\n",
        "    # base_model = InceptionV3(\n",
        "    # weights='imagenet',\n",
        "    # include_top=True)\n",
        "    \n",
        "    \n",
        "    # We'll extract features at the final pool layer.\n",
        "    # model = Model(\n",
        "    #     inputs=base_model.input,\n",
        "    #     outputs=base_model.get_layer('avg_pool').output)\n",
        "    # # model.save('inception.h5')\n",
        "\n",
        "    base_model = load_model('./drive/My Drive/imageclassifier3_1.h5')\n",
        "\n",
        "    layer_name = 'flatten_1'\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.get_layer(layer_name).output)\n",
        "\n",
        "    # Getting the data\n",
        "    df = get_data('anomalydetection/Data/data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    # df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    train_video_index = make_dataset(train, model,label_index, \"train\")\n",
        "    test_video_index = make_dataset(test, model,label_index,\"test\")\n",
        "    return (train_video_index, test_video_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLx9uiODN6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "#def clean_data(df):\n",
        "#    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "#    df = df[mask]`\n",
        "#    return df\n",
        "\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (299,299))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def video_to_frame(row, model,label_index, phase, not_created):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    index = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(\"anomalydetection/Data\",\"anomaly_dataset\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".mp4\")) \n",
        "    #print(str(row[\"class\"].iloc[0]))\n",
        "    #print(str(row[\"video_name\"].iloc[0]))\n",
        "    frameno = 1\n",
        "    imgs = []\n",
        "    length = 0\n",
        "    seq = 12\n",
        "    while (cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if length < seq:\n",
        "          if frameno % 10 == 0:\n",
        "            frameno = frameno + 1\n",
        "            frame = preprocess_image(frame)\n",
        "            frame = image.img_to_array(frame)\n",
        "            frame = frame / 255\n",
        "            # features = model.predict(frame)\n",
        "            imgs.append(frame)\n",
        "            length = length + 1\n",
        "          else:\n",
        "            frameno = frameno + 1\n",
        "        else:\n",
        "          seq = seq + 12\n",
        "          imgs1 = np.array(imgs)\n",
        "          features = model.predict(imgs1)\n",
        "          input_f.append(features)\n",
        "          output_y.append(label_index)\n",
        "          del imgs[:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if not_created:\n",
        "      f = h5py.File(phase+'_4'+'.h5', 'w')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        f.create_dataset(phase, data=input_f1, maxshape=(None, 12, 2048))\n",
        "        f.create_dataset(phase+\"_labels\", data=output_y1, maxshape=(None,))\n",
        "        f.close()\n",
        "    else:\n",
        "      hf = h5py.File(phase+'_4'+'.h5', 'a')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        hf[phase].resize((hf[phase].shape[0] + input_f1.shape[0]), axis = 0)\n",
        "        hf[phase][-input_f1.shape[0]:] = input_f1\n",
        "\n",
        "        hf[phase+\"_labels\"].resize((hf[phase+\"_labels\"].shape[0] + output_y1.shape[0]), axis = 0)\n",
        "        hf[phase+\"_labels\"][-output_y1.shape[0]:] = output_y1\n",
        "        hf.close()\n",
        "\n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "    del imgs[:]\n",
        "    cap.release()\n",
        "    return index\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def make_dataset(data, model,label_index, phase):\n",
        "    video_index = [0]\n",
        "    required_classes = [\"Abuse\", \"Arson\",\"Assault\", \"Burglary\", \"Fighting\", \"normal\"]\n",
        "   \n",
        "    not_created = True\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        "            index = required_classes.index(str(data.iloc[[i]][\"class\"].iloc[0]))\n",
        "            # label_index = np.zeros((4))\n",
        "            # label_index[index] = 1\n",
        "            label_index = index\n",
        "            index = video_to_frame(data.iloc[[i]], model,label_index, phase, not_created)\n",
        "            real_index = video_index[-1] + index\n",
        "            video_index.append(real_index)\n",
        "            if real_index > 0:\n",
        "              not_created = False\n",
        "\n",
        "    return video_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa9JuDGnFJbl",
        "colab_type": "code",
        "outputId": "cd27fc8e-3154-427b-ff01-82fb02af1a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "train_video_index, test_video_index = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 192/192 [02:24<00:00,  1.42it/s]\n",
            "100%|██████████| 39/39 [00:34<00:00,  1.33s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZtdRrCFKRp",
        "colab_type": "code",
        "outputId": "5aa51380-aa5f-45d6-b052-fcd9cc7de3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "x_train = HDF5Matrix('train_4.h5', 'train')\n",
        "y_train = HDF5Matrix('train_4.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_4.h5', 'test')\n",
        "y_test = HDF5Matrix('test_4.h5', 'test_labels')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[783])\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test[93])\n",
        "print(train_video_index)\n",
        "print(test_video_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1496, 12, 2048)\n",
            "(1496,)\n",
            "2\n",
            "(378, 12, 2048)\n",
            "(378,)\n",
            "0\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzPj0HpcTVQ6",
        "colab_type": "code",
        "outputId": "9c74cb79-68c8-46fd-c735-ca669b51c6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "y_test1 = np.array(y_test)\n",
        "print(y_test1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
            " 5 5 5 5 5 5 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdbPFBLhFP2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predominantly.\"\"\"\n",
        "    input_shape = (12, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(2048), input_shape=input_shape))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "    #model.add(Dense(10, activation='softmax'))\"\"\"\n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "    \n",
        "    callback_list = [checkpoint]\n",
        "    \n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "    #return model, callback_list\n",
        "    #model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "    #          loss = 'categorical_crossentropy',\n",
        "    #        metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdZFdLaqWk5T",
        "colab_type": "code",
        "outputId": "08824c7c-10b1-4fce-aa2f-23e751236399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "lstm = lstm()\n",
        "lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 4096)              67125248  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 69,225,990\n",
            "Trainable params: 69,225,990\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZXBVoGWqz5",
        "colab_type": "code",
        "outputId": "cba0c3d7-7e46-4b52-a5e7-5907e4e1c43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lstm.fit(x_train, y_train, batch_size = 16, epochs = 25,verbose = 2, validation_data = (x_test, y_test), shuffle = 'batch')\n",
        "lstm.save(\"Anomaly_Recognition_4classes.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1496 samples, validate on 378 samples\n",
            "Epoch 1/25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 15s - loss: 2.9363 - acc: 0.2045 - val_loss: 1.6075 - val_acc: 0.2751\n",
            "Epoch 2/25\n",
            " - 10s - loss: 2.3674 - acc: 0.2413 - val_loss: 1.5490 - val_acc: 0.3677\n",
            "Epoch 3/25\n",
            " - 10s - loss: 2.1487 - acc: 0.2634 - val_loss: 1.6171 - val_acc: 0.2593\n",
            "Epoch 4/25\n",
            " - 10s - loss: 1.9003 - acc: 0.3068 - val_loss: 1.5672 - val_acc: 0.5476\n",
            "Epoch 5/25\n",
            " - 10s - loss: 1.8464 - acc: 0.3148 - val_loss: 1.5797 - val_acc: 0.3545\n",
            "Epoch 6/25\n",
            " - 10s - loss: 1.7633 - acc: 0.3329 - val_loss: 1.5437 - val_acc: 0.5132\n",
            "Epoch 7/25\n",
            " - 10s - loss: 1.7038 - acc: 0.3523 - val_loss: 1.5704 - val_acc: 0.4815\n",
            "Epoch 8/25\n",
            " - 10s - loss: 1.6501 - acc: 0.3797 - val_loss: 1.5561 - val_acc: 0.5238\n",
            "Epoch 9/25\n",
            " - 10s - loss: 1.5972 - acc: 0.3636 - val_loss: 1.5410 - val_acc: 0.4471\n",
            "Epoch 10/25\n",
            " - 10s - loss: 1.5525 - acc: 0.3877 - val_loss: 1.5076 - val_acc: 0.5317\n",
            "Epoch 11/25\n",
            " - 10s - loss: 1.5107 - acc: 0.4398 - val_loss: 1.5317 - val_acc: 0.3598\n",
            "Epoch 12/25\n",
            " - 10s - loss: 1.4684 - acc: 0.4285 - val_loss: 1.5358 - val_acc: 0.3360\n",
            "Epoch 13/25\n",
            " - 10s - loss: 1.4440 - acc: 0.4472 - val_loss: 1.4938 - val_acc: 0.5185\n",
            "Epoch 14/25\n",
            " - 10s - loss: 1.3925 - acc: 0.4659 - val_loss: 1.4762 - val_acc: 0.4048\n",
            "Epoch 15/25\n",
            " - 10s - loss: 1.3741 - acc: 0.4713 - val_loss: 1.4563 - val_acc: 0.4974\n",
            "Epoch 16/25\n",
            " - 10s - loss: 1.3344 - acc: 0.4940 - val_loss: 1.4162 - val_acc: 0.5503\n",
            "Epoch 17/25\n",
            " - 10s - loss: 1.2924 - acc: 0.5214 - val_loss: 1.4679 - val_acc: 0.5159\n",
            "Epoch 18/25\n",
            " - 10s - loss: 1.2429 - acc: 0.5314 - val_loss: 1.4205 - val_acc: 0.5079\n",
            "Epoch 19/25\n",
            " - 10s - loss: 1.2744 - acc: 0.5207 - val_loss: 1.4528 - val_acc: 0.4947\n",
            "Epoch 20/25\n",
            " - 10s - loss: 1.1934 - acc: 0.5548 - val_loss: 1.4118 - val_acc: 0.5450\n",
            "Epoch 21/25\n",
            " - 10s - loss: 1.1487 - acc: 0.5735 - val_loss: 1.3894 - val_acc: 0.5476\n",
            "Epoch 22/25\n",
            " - 10s - loss: 1.1498 - acc: 0.5655 - val_loss: 1.3742 - val_acc: 0.5291\n",
            "Epoch 23/25\n",
            " - 10s - loss: 1.1368 - acc: 0.5775 - val_loss: 1.3798 - val_acc: 0.4762\n",
            "Epoch 24/25\n",
            " - 10s - loss: 1.1009 - acc: 0.5936 - val_loss: 1.3963 - val_acc: 0.4788\n",
            "Epoch 25/25\n",
            " - 10s - loss: 1.0820 - acc: 0.5989 - val_loss: 1.3843 - val_acc: 0.5079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdRDgRq0Wxcp",
        "colab_type": "code",
        "outputId": "a4c96ff6-b5af-400a-e6f7-ad1ddd037e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = lstm.evaluate(x_test, y_test, batch_size=16)\n",
        "print(\"accuracy: \", test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "378/378 [==============================] - 1s 2ms/step\n",
            "accuracy:  0.5079365079365079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCWp-Fv5lgdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}