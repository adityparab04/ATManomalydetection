{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anomaly_4_classes_using TransferLearningInceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogaveera/atmanomalydetection/blob/master/anomaly_4_classes_using_TransferLearningInceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTt3IU9RC8nB",
        "colab_type": "code",
        "outputId": "02fb1b89-326d-4936-faea-3d09f70b46a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "\n",
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "train_video_index = []\n",
        "test_video_index = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMjx_CNDGIt",
        "colab_type": "code",
        "outputId": "c4dd66d5-a58c-4cc3-b2f6-82687ecf18f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/Mogaveera/anomalydetection.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'anomalydetection'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 250 (delta 0), reused 5 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (250/250), 733.86 MiB | 57.49 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "Checking out files: 100% (233/233), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1jogWNDKUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Get model with pretrained weights.\n",
        "    base_model = InceptionV3(\n",
        "    weights='imagenet',\n",
        "    include_top=True)\n",
        "    \n",
        "    \n",
        "    # We'll extract features at the final pool layer.\n",
        "    model = Model(\n",
        "        inputs=base_model.input,\n",
        "        outputs=base_model.get_layer('avg_pool').output)\n",
        "    model.save('inception.h5')\n",
        "\n",
        "    # Getting the data\n",
        "    df = get_data('anomalydetection/Data/data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    # df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    train_video_index = make_dataset(train, model,label_index, \"train\")\n",
        "    test_video_index = make_dataset(test, model,label_index,\"test\")\n",
        "    return (train_video_index, test_video_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLx9uiODN6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "#def clean_data(df):\n",
        "#    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "#    df = df[mask]`\n",
        "#    return df\n",
        "\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (299,299))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def video_to_frame(row, model,label_index, phase, not_created):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    index = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(\"anomalydetection/Data\",\"anomaly_dataset\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".mp4\")) \n",
        "    #print(str(row[\"class\"].iloc[0]))\n",
        "    #print(str(row[\"video_name\"].iloc[0]))\n",
        "    frameno = 1\n",
        "    imgs = []\n",
        "    length = 0\n",
        "    seq = 12\n",
        "    while (cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if length < seq:\n",
        "          if frameno % 10 == 0:\n",
        "            frameno = frameno + 1\n",
        "            frame = preprocess_image(frame)\n",
        "            frame = image.img_to_array(frame)\n",
        "            frame = frame / 255\n",
        "            # features = model.predict(frame)\n",
        "            imgs.append(frame)\n",
        "            length = length + 1\n",
        "          else:\n",
        "            frameno = frameno + 1\n",
        "        else:\n",
        "          seq = seq + 12\n",
        "          imgs1 = np.array(imgs)\n",
        "          features = model.predict(imgs1)\n",
        "          input_f.append(features)\n",
        "          output_y.append(label_index)\n",
        "          del imgs[:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if not_created:\n",
        "      f = h5py.File(phase+'_4'+'.h5', 'w')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        f.create_dataset(phase, data=input_f1, maxshape=(None, 12, 2048))\n",
        "        f.create_dataset(phase+\"_labels\", data=output_y1, maxshape=(None,))\n",
        "        f.close()\n",
        "    else:\n",
        "      hf = h5py.File(phase+'_4'+'.h5', 'a')\n",
        "      input_f1 = np.array(input_f)\n",
        "      output_y1 = np.array(output_y)\n",
        "      index = input_f1.shape[0]\n",
        "      if index > 0:\n",
        "        hf[phase].resize((hf[phase].shape[0] + input_f1.shape[0]), axis = 0)\n",
        "        hf[phase][-input_f1.shape[0]:] = input_f1\n",
        "\n",
        "        hf[phase+\"_labels\"].resize((hf[phase+\"_labels\"].shape[0] + output_y1.shape[0]), axis = 0)\n",
        "        hf[phase+\"_labels\"][-output_y1.shape[0]:] = output_y1\n",
        "        hf.close()\n",
        "\n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "    del imgs[:]\n",
        "    cap.release()\n",
        "    return index\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def make_dataset(data, model,label_index, phase):\n",
        "    video_index = [0]\n",
        "    required_classes = [\"Arson\", \"Burglary\", \"Fighting\", \"normal\"]\n",
        "   \n",
        "    not_created = True\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        "            index = required_classes.index(str(data.iloc[[i]][\"class\"].iloc[0]))\n",
        "            label_index = index\n",
        "            index = video_to_frame(data.iloc[[i]], model,label_index, phase, not_created)\n",
        "            real_index = video_index[-1] + index\n",
        "            video_index.append(real_index)\n",
        "            if real_index > 0:\n",
        "              not_created = False\n",
        "\n",
        "    return video_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa9JuDGnFJbl",
        "colab_type": "code",
        "outputId": "c3f77391-c975-441c-8d0a-d5ba41b635e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "train_video_index, test_video_index = main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 192/192 [01:36<00:00,  1.16it/s]\n",
            "100%|██████████| 39/39 [00:25<00:00,  1.60s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZtdRrCFKRp",
        "colab_type": "code",
        "outputId": "075f887a-2718-49e0-ecca-c2e1094c205f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "x_train = HDF5Matrix('train_4.h5', 'train')\n",
        "y_train = HDF5Matrix('train_4.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_4.h5', 'test')\n",
        "y_test = HDF5Matrix('test_4.h5', 'test_labels')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[783])\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test[93])\n",
        "print(train_video_index)\n",
        "print(test_video_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(786, 12, 2048)\n",
            "(786,)\n",
            "3\n",
            "(225, 12, 2048)\n",
            "(225,)\n",
            "3\n",
            "[0, 0, 3, 6, 10, 13, 16, 20, 21, 23, 26, 29, 32, 37, 42, 46, 49, 57, 61, 64, 68, 68, 73, 78, 83, 86, 90, 94, 96, 99, 101, 104, 105, 106, 107, 112, 114, 118, 123, 124, 128, 131, 132, 134, 136, 145, 150, 156, 160, 164, 167, 171, 173, 176, 177, 180, 197, 198, 208, 211, 222, 226, 241, 247, 248, 259, 277, 283, 292, 295, 330, 348, 374, 390, 423, 428, 435, 436, 453, 457, 477, 482, 485, 504, 513, 519, 522, 530, 542, 545, 554, 577, 585, 589, 598, 611, 621, 631, 637, 650, 658, 677, 689, 704, 706, 714, 722, 727, 730, 737, 751, 753, 759, 775, 780, 786]\n",
            "[0, 1, 3, 4, 5, 9, 12, 17, 20, 26, 33, 38, 45, 52, 64, 69, 87, 93, 108, 165, 206, 215, 218, 225]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQl5HUB_BLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test1 = np.array(y_test)\n",
        "# print(y_test1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdbPFBLhFP2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predominantly.\"\"\"\n",
        "    input_shape = (12, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(2048), input_shape=input_shape))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    #model.add(Dense(10, activation='softmax'))\"\"\"\n",
        "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=2,\n",
        "    write_graph=True\n",
        "    )\n",
        "    \n",
        "    callback_list = [checkpoint]\n",
        "    \n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "    #return model, callback_list\n",
        "    #model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "    #          loss = 'categorical_crossentropy',\n",
        "    #        metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdZFdLaqWk5T",
        "colab_type": "code",
        "outputId": "ab371549-85db-4383-b57e-abbf44fca018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "lstm = lstm()\n",
        "lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 4096)              67125248  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 69,224,964\n",
            "Trainable params: 69,224,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZXBVoGWqz5",
        "colab_type": "code",
        "outputId": "44be0cdc-c3c5-4882-f535-b37efb4e196b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "lstm.fit(x_train, y_train, batch_size = 16, epochs = 20,verbose = 2, validation_data = (x_test, y_test), shuffle = 'batch')\n",
        "lstm.save(\"Anomaly_Recognition_4classes.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 786 samples, validate on 225 samples\n",
            "Epoch 1/20\n",
            " - 12s - loss: 1.8754 - acc: 0.3053 - val_loss: 1.4708 - val_acc: 0.2444\n",
            "Epoch 2/20\n",
            " - 5s - loss: 1.4749 - acc: 0.4211 - val_loss: 1.3287 - val_acc: 0.2444\n",
            "Epoch 3/20\n",
            " - 5s - loss: 1.3435 - acc: 0.4288 - val_loss: 1.1787 - val_acc: 0.2444\n",
            "Epoch 4/20\n",
            " - 5s - loss: 1.2959 - acc: 0.4809 - val_loss: 1.0931 - val_acc: 0.3378\n",
            "Epoch 5/20\n",
            " - 5s - loss: 1.1018 - acc: 0.5433 - val_loss: 1.0835 - val_acc: 0.3422\n",
            "Epoch 6/20\n",
            " - 5s - loss: 1.0783 - acc: 0.5573 - val_loss: 0.9972 - val_acc: 0.5422\n",
            "Epoch 7/20\n",
            " - 5s - loss: 0.9864 - acc: 0.6069 - val_loss: 0.9567 - val_acc: 0.6044\n",
            "Epoch 8/20\n",
            " - 5s - loss: 0.9146 - acc: 0.6285 - val_loss: 0.8324 - val_acc: 0.7778\n",
            "Epoch 9/20\n",
            " - 5s - loss: 0.8710 - acc: 0.6641 - val_loss: 0.7762 - val_acc: 0.8133\n",
            "Epoch 10/20\n",
            " - 5s - loss: 0.8417 - acc: 0.6743 - val_loss: 0.8221 - val_acc: 0.6978\n",
            "Epoch 11/20\n",
            " - 5s - loss: 0.7162 - acc: 0.7328 - val_loss: 0.7725 - val_acc: 0.7111\n",
            "Epoch 12/20\n",
            " - 5s - loss: 0.6861 - acc: 0.7583 - val_loss: 0.7366 - val_acc: 0.7111\n",
            "Epoch 13/20\n",
            " - 5s - loss: 0.6365 - acc: 0.7595 - val_loss: 0.7114 - val_acc: 0.7156\n",
            "Epoch 14/20\n",
            " - 5s - loss: 0.5714 - acc: 0.8028 - val_loss: 0.6482 - val_acc: 0.7467\n",
            "Epoch 15/20\n",
            " - 5s - loss: 0.5176 - acc: 0.8053 - val_loss: 0.5757 - val_acc: 0.7911\n",
            "Epoch 16/20\n",
            " - 5s - loss: 0.4692 - acc: 0.8422 - val_loss: 0.5480 - val_acc: 0.7956\n",
            "Epoch 17/20\n",
            " - 5s - loss: 0.4285 - acc: 0.8448 - val_loss: 0.5966 - val_acc: 0.7289\n",
            "Epoch 18/20\n",
            " - 5s - loss: 0.4063 - acc: 0.8499 - val_loss: 0.4951 - val_acc: 0.8089\n",
            "Epoch 19/20\n",
            " - 5s - loss: 0.3981 - acc: 0.8702 - val_loss: 0.4776 - val_acc: 0.8267\n",
            "Epoch 20/20\n",
            " - 5s - loss: 0.3314 - acc: 0.8817 - val_loss: 0.4544 - val_acc: 0.8489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdRDgRq0Wxcp",
        "colab_type": "code",
        "outputId": "5b26703b-2196-45bc-b42f-bd20b79ca03e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = lstm.evaluate(x_test, y_test, batch_size=16)\n",
        "print(\"accuracy: \", test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 0s 2ms/step\n",
            "accuracy:  0.8488888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSpzu6HD-8oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}